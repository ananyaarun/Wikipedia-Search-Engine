import sys
import getopt
import time
import os
import re
import xml.sax
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk.stem import WordNetLemmatizer
import Stemmer
from collections import defaultdict

sno = SnowballStemmer('english')
stemmer = Stemmer.Stemmer('english')
stop_words = set(stopwords.words('english'))
temp ='-'

mydict =  {
  "t": 0,
  "b": 1,
  "c": 2,
  "i": 3,
  "r": 4,
  "e": 5 
}

ftype = ['t', 'b', 'c', 'i', 'r', 'e']
fscore = [0.5,0.3,0.2,0.2,0.1,0.05]
fquery = ["t:", "b:", "c:", "i:",  "r:", "l:"]

limit = 10000
start_words = open('./index/start_words.txt', 'r')
words = file.readlines()



def getlist(word):
    ind = bisect.bisect_right(words, word) - 1
    if ind == -1:
        return ''
    file = open('./index/' + str(ind) + '.txt', 'r')
    line = file.readline().strip('\n')
    wrd = line.split(":")[0]
    while line:
        if wrd == word:
            return line.split(":")[1]
        line = file.readline().strip('\n')
        wrd = line.split(":")[0]
    return ''


def fetch_title(doc_no):
    off = doc_no // 100000
    file = open("./titles" + str(off) + '.txt')
    return file.readlines()[doc_no % threshold].strip('\n')


def tokenize1(text):
    global temp
    tokensAndFields = []
    for token in text:
        tokenAndField = token.split(':', 1)
        if len(tokenAndField) == 1:
            tokensAndFields.append([tokenAndField[0], temp])
        else:
            tokenAndField[0]=tokenAndField[0].lower()
            tokenAndField[0] = re.split(r'[^A-Za-z0-9]+', tokenAndField[0])
            temp = tokenAndField[0]
            tokensAndFields.append([tokenAndField[1], tokenAndField[0]])
    return tokensAndFields


def removeStopWords1(tokensAndFields):
    tokens = [token for token in tokensAndFields if not token[0] in stop_words]
    return tokens


def stem1(tokensAndFields):
    stemmedTokens = [[sno.stem(token[0]), token[1]] for token in tokensAndFields]
    return stemmedTokens


def tokenize(text):
    tokens = re.split(r'[^A-Za-z0-9]+', text)
    tot += len(tokens)
    return tokens


def removeStopWords(tokens):
    words = []
    for token in tokens:
        if token not in stop_words:
            words.append(token)
    return words


def stem(tokens):
    return stemmer.stemWords(tokens)



def ranking(doc, idcs, plist):
    doc_no = doc.split('d')[0]
    for i in range(6):
        if i in idcs:

            if ftype[i] not in doc:
                continue
            term_freq[doc_no][i+1] += 1
            tf = count(doc, ftype[i])
            idf = math.log2(total_docs / doc.count(ftype[i]))
            term_freq[doc_no][7] += tf * idf *
    term_freq[doc_no][0] = max(term_freq[doc_no][1:7])




if __name__ == '__main__':

    path_to_file = sys.argv[1]

    file = open(path_to_file, 'r')

    lines = file.readlines()

    for line in lines:
        line = line.lower()
        query = line.split(',')
        num = query[0]
        query = query[1].strip('\n')
        query = query[1:]
        # print(query)
        # tokens = tokenize(query)
        # tokens = removeStopWords(tokens)
        # tokens = stem(tokens)
        # print(num)
        termfreq = defaultdict(lambda : [0] * 8)
        chk = 0 
        idx = []
        for i in fquery:
            if i in query:
                idx.append(mydict[i])

        if ':' in query:
            chk = 1

        if chk == 0:
            tokens = tokenize(query)
            tokens = removeStopWords(tokens)
            tokens = stem(tokens)

            for tok in tokens:
                posting = getlist(tok)
                if len(posting) == 0:
                    continue
                docc = posting.split(' ')
                docc = docc[1:]

                for doc in docc:
                    ranking(doc,[0,1],posting)

        else:
            querys = query.split(' ')
            temp = 't'

            for q in querys:
                ind = 1
                if 't:' in q:
                    ind = 0
                elif 'b:' in q:
                    ind = 1
                elif 'c:' in q:
                    ind = 2
                elif 'i:' in q:
                    ind = 3
                elif 'r:' in q:
                    ind = 4
                elif 'e:' in q:
                    ind = 5

                if ':' in q:
                    q = q.split(':')[1]

                tokens = tokenize(q)
                tokens = removeStopWords(tokens)
                tokens = stem(tokens)

                posting = getlist(tokens)
                if len(posting) == 0:
                    continue;
                docc = posting.split(' ')
                docc = docc[1:]

                for doc in docc:
                    ranking(doc,[ind],posting)









    
    file.close()

